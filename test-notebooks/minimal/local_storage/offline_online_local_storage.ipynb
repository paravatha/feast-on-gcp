{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Ride Hailing Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Feast Data Flow](https://raw.githubusercontent.com/feast-dev/feast/master/examples/minimal/images/data-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this quick start, we will:\n",
    "1. Register two driver features, one for driver statistics, the other for driver trips. Driver statistics are updated on daily basis, whereas driver trips are updated in real time.\n",
    "2. Creates a driver dataset, then use Feast SDK to retrieve the features corresponding to these drivers from an offline store.\n",
    "3. Store the features in an online store (Redis), and retrieve the features via Feast SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features Registry (Feast Core)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurations can be provided in three different methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using environmental variables\n",
    "# os.environ[\"FEAST_CORE_URL\"] = \"core:6565\"\n",
    "# os.environ[\"FEAST_SERVING_URL\"] = \"online_serving:6566\"\n",
    "\n",
    "# Provide a map during client initialization\n",
    "# options = {\n",
    "#     \"CORE_URL\": \"core:6565\",\n",
    "#     \"SERVING_URL\": \"online_serving:6566\", \n",
    "# }\n",
    "# client = Client(options)\n",
    "\n",
    "# As keyword arguments, without the `FEAST` prefix\n",
    "# client = Client(core_url=\"core:6565\", serving_url=\"online_serving:6566\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are following the quick start guide, all required configurations to follow the remainder of the tutorial should have been setup, in the form of environmental variables, as showned below. The configuration values may differ depending on the environment. For a full list of configurable values and explanation, please refer to the user guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'FEAST_CORE_URL': 'feast-release-feast-core:6565',\n",
      " 'FEAST_HISTORICAL_SERVING_URL': 'feast-release-feast-batch-serving:6566',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT': 'tcp://10.79.249.237:80',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_6565_TCP': 'tcp://10.79.249.237:6565',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_6565_TCP_ADDR': '10.79.249.237',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_6565_TCP_PORT': '6565',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_6565_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_80_TCP': 'tcp://10.79.249.237:80',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_80_TCP_ADDR': '10.79.249.237',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_80_TCP_PORT': '80',\n",
      " 'FEAST_RELEASE_FEAST_CORE_PORT_80_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_FEAST_CORE_SERVICE_HOST': '10.79.249.237',\n",
      " 'FEAST_RELEASE_FEAST_CORE_SERVICE_PORT': '80',\n",
      " 'FEAST_RELEASE_FEAST_CORE_SERVICE_PORT_GRPC': '6565',\n",
      " 'FEAST_RELEASE_FEAST_CORE_SERVICE_PORT_HTTP': '80',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT': 'tcp://10.79.243.227:80',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_6568_TCP': 'tcp://10.79.243.227:6568',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_6568_TCP_ADDR': '10.79.243.227',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_6568_TCP_PORT': '6568',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_6568_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_80_TCP': 'tcp://10.79.243.227:80',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_80_TCP_ADDR': '10.79.243.227',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_80_TCP_PORT': '80',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_PORT_80_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_SERVICE_HOST': '10.79.243.227',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_SERVICE_PORT': '80',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_SERVICE_PORT_GRPC': '6568',\n",
      " 'FEAST_RELEASE_FEAST_JOBSERVICE_SERVICE_PORT_HTTP': '80',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT': 'tcp://10.79.246.157:80',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_6566_TCP': 'tcp://10.79.246.157:6566',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_6566_TCP_ADDR': '10.79.246.157',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_6566_TCP_PORT': '6566',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_6566_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_80_TCP': 'tcp://10.79.246.157:80',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_80_TCP_ADDR': '10.79.246.157',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_80_TCP_PORT': '80',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_PORT_80_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_SERVICE_HOST': '10.79.246.157',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_SERVICE_PORT': '80',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_SERVICE_PORT_GRPC': '6566',\n",
      " 'FEAST_RELEASE_FEAST_ONLINE_SERVING_SERVICE_PORT_HTTP': '80',\n",
      " 'FEAST_RELEASE_GRAFANA_PORT': 'tcp://10.79.251.162:80',\n",
      " 'FEAST_RELEASE_GRAFANA_PORT_80_TCP': 'tcp://10.79.251.162:80',\n",
      " 'FEAST_RELEASE_GRAFANA_PORT_80_TCP_ADDR': '10.79.251.162',\n",
      " 'FEAST_RELEASE_GRAFANA_PORT_80_TCP_PORT': '80',\n",
      " 'FEAST_RELEASE_GRAFANA_PORT_80_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_GRAFANA_SERVICE_HOST': '10.79.251.162',\n",
      " 'FEAST_RELEASE_GRAFANA_SERVICE_PORT': '80',\n",
      " 'FEAST_RELEASE_GRAFANA_SERVICE_PORT_SERVICE': '80',\n",
      " 'FEAST_RELEASE_KAFKA_PORT': 'tcp://10.79.242.9:9092',\n",
      " 'FEAST_RELEASE_KAFKA_PORT_9092_TCP': 'tcp://10.79.242.9:9092',\n",
      " 'FEAST_RELEASE_KAFKA_PORT_9092_TCP_ADDR': '10.79.242.9',\n",
      " 'FEAST_RELEASE_KAFKA_PORT_9092_TCP_PORT': '9092',\n",
      " 'FEAST_RELEASE_KAFKA_PORT_9092_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_KAFKA_SERVICE_HOST': '10.79.242.9',\n",
      " 'FEAST_RELEASE_KAFKA_SERVICE_PORT': '9092',\n",
      " 'FEAST_RELEASE_KAFKA_SERVICE_PORT_TCP_CLIENT': '9092',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_PORT': 'tcp://10.79.246.15:8080',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_PORT_8080_TCP': 'tcp://10.79.246.15:8080',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_PORT_8080_TCP_ADDR': '10.79.246.15',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_PORT_8080_TCP_PORT': '8080',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_PORT_8080_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_SERVICE_HOST': '10.79.246.15',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_SERVICE_PORT': '8080',\n",
      " 'FEAST_RELEASE_KUBE_STATE_METRICS_SERVICE_PORT_HTTP': '8080',\n",
      " 'FEAST_RELEASE_POSTGRESQL_PORT': 'tcp://10.79.248.163:5432',\n",
      " 'FEAST_RELEASE_POSTGRESQL_PORT_5432_TCP': 'tcp://10.79.248.163:5432',\n",
      " 'FEAST_RELEASE_POSTGRESQL_PORT_5432_TCP_ADDR': '10.79.248.163',\n",
      " 'FEAST_RELEASE_POSTGRESQL_PORT_5432_TCP_PORT': '5432',\n",
      " 'FEAST_RELEASE_POSTGRESQL_PORT_5432_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_POSTGRESQL_SERVICE_HOST': '10.79.248.163',\n",
      " 'FEAST_RELEASE_POSTGRESQL_SERVICE_PORT': '5432',\n",
      " 'FEAST_RELEASE_POSTGRESQL_SERVICE_PORT_TCP_POSTGRESQL': '5432',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_PORT': 'tcp://10.79.255.68:80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_PORT_80_TCP': 'tcp://10.79.255.68:80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_PORT_80_TCP_ADDR': '10.79.255.68',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_PORT_80_TCP_PORT': '80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_PORT_80_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_SERVICE_HOST': '10.79.255.68',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_SERVICE_PORT': '80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_ALERTMANAGER_SERVICE_PORT_HTTP': '80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_PORT': 'tcp://10.79.243.175:9091',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_PORT_9091_TCP': 'tcp://10.79.243.175:9091',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_PORT_9091_TCP_ADDR': '10.79.243.175',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_PORT_9091_TCP_PORT': '9091',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_PORT_9091_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_SERVICE_HOST': '10.79.243.175',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_SERVICE_PORT': '9091',\n",
      " 'FEAST_RELEASE_PROMETHEUS_PUSHGATEWAY_SERVICE_PORT_HTTP': '9091',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_PORT': 'tcp://10.79.249.252:80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_PORT_80_TCP': 'tcp://10.79.249.252:80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_PORT_80_TCP_ADDR': '10.79.249.252',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_PORT_80_TCP_PORT': '80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_PORT_80_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_SERVICE_HOST': '10.79.249.252',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_SERVICE_PORT': '80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_SERVER_SERVICE_PORT_HTTP': '80',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT': 'tcp://10.79.251.29:9102',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9102_TCP': 'tcp://10.79.251.29:9102',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9102_TCP_ADDR': '10.79.251.29',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9102_TCP_PORT': '9102',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9102_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9125_TCP': 'tcp://10.79.251.29:9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9125_TCP_ADDR': '10.79.251.29',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9125_TCP_PORT': '9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_PORT_9125_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_SERVICE_HOST': '10.79.251.29',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_SERVICE_PORT': '9102',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_SERVICE_PORT_METRICS': '9102',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_SERVICE_PORT_STATSD_TCP': '9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_PORT': 'udp://10.79.252.115:9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_PORT_9125_UDP': 'udp://10.79.252.115:9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_PORT_9125_UDP_ADDR': '10.79.252.115',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_PORT_9125_UDP_PORT': '9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_PORT_9125_UDP_PROTO': 'udp',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_SERVICE_HOST': '10.79.252.115',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_SERVICE_PORT': '9125',\n",
      " 'FEAST_RELEASE_PROMETHEUS_STATSD_EXPORTER_UDP_SERVICE_PORT_STATSD_UDP': '9125',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_PORT': 'tcp://10.79.245.82:6379',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_PORT_6379_TCP': 'tcp://10.79.245.82:6379',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_PORT_6379_TCP_ADDR': '10.79.245.82',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_PORT_6379_TCP_PORT': '6379',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_PORT_6379_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_SERVICE_HOST': '10.79.245.82',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_SERVICE_PORT': '6379',\n",
      " 'FEAST_RELEASE_REDIS_MASTER_SERVICE_PORT_REDIS': '6379',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_PORT': 'tcp://10.79.253.156:6379',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_PORT_6379_TCP': 'tcp://10.79.253.156:6379',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_PORT_6379_TCP_ADDR': '10.79.253.156',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_PORT_6379_TCP_PORT': '6379',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_PORT_6379_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_SERVICE_HOST': '10.79.253.156',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_SERVICE_PORT': '6379',\n",
      " 'FEAST_RELEASE_REDIS_SLAVE_SERVICE_PORT_REDIS': '6379',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT': 'tcp://10.79.250.37:2181',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2181_TCP': 'tcp://10.79.250.37:2181',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2181_TCP_ADDR': '10.79.250.37',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2181_TCP_PORT': '2181',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2181_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2888_TCP': 'tcp://10.79.250.37:2888',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2888_TCP_ADDR': '10.79.250.37',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2888_TCP_PORT': '2888',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_2888_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_3888_TCP': 'tcp://10.79.250.37:3888',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_3888_TCP_ADDR': '10.79.250.37',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_3888_TCP_PORT': '3888',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_PORT_3888_TCP_PROTO': 'tcp',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_SERVICE_HOST': '10.79.250.37',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_SERVICE_PORT': '2181',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_SERVICE_PORT_FOLLOWER': '2888',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_SERVICE_PORT_TCP_CLIENT': '2181',\n",
      " 'FEAST_RELEASE_ZOOKEEPER_SERVICE_PORT_TCP_ELECTION': '3888',\n",
      " 'FEAST_SERVING_URL': 'feast-release-feast-online-serving:6566',\n",
      " 'FEAST_SPARK_HOME': '/usr/local/spark',\n",
      " 'FEAST_SPARK_LAUNCHER': 'standalone',\n",
      " 'FEAST_SPARK_STANDALONE_MASTER': 'local[*]'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "pprint({key: value for key, value in os.environ.items() if key.startswith(\"FEAST_\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports and Feast Client initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from feast import Client, Feature, Entity, ValueType, FeatureTable\n",
    "from feast.data_source import FileSource, KafkaSource\n",
    "from feast.data_format import ParquetFormat, AvroFormat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#client = Client()\n",
    "client = Client(redis_host=os.environ[\"FEAST_RELEASE_REDIS_MASTER_SERVICE_HOST\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare Features and Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entity defines the primary key(s) associated with one or more feature tables. The entity must be registered before declaring the associated feature tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_id = Entity(name=\"driver_id\", description=\"Driver identifier\", value_type=ValueType.INT64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily updated features \n",
    "acc_rate = Feature(\"acc_rate\", ValueType.FLOAT)\n",
    "conv_rate = Feature(\"conv_rate\", ValueType.FLOAT)\n",
    "avg_daily_trips = Feature(\"avg_daily_trips\", ValueType.INT32)\n",
    "\n",
    "# Real-time updated features\n",
    "trips_today = Feature(\"trips_today\", ValueType.INT32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "FeatureTable(\n",
    "    name = \"driver_statistics\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = [\n",
    "        acc_rate,\n",
    "        conv_rate,\n",
    "        avg_daily_trips\n",
    "    ]\n",
    "    ...\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "FeatureTable(\n",
    "    name = \"driver_trips\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = [\n",
    "        trips_today\n",
    "    ]\n",
    "    ...\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Features Join](./images/features-join.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "FeatureTable(\n",
    "    ...,\n",
    "    batch_source=FileSource(  # Required\n",
    "        file_format=ParquetFormat(),\n",
    "        file_url=\"gs://feast-demo-data-lake\",\n",
    "        ...\n",
    "    ),\n",
    "    stream_source=KafkaSource(  # Optional\n",
    "        bootstrap_servers=\"...\",\n",
    "        topic=\"driver_trips\",\n",
    "        ...\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature tables group the features together and describe how they can be retrieved. The following examples assume that the feature tables are stored on the local file system, and is accessible from the Spark cluster. If you have setup a GCP service account, you may use GCS instead as the file source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batch_source` defines where the historical features are stored. It is also possible to have an optional `stream_source`, which the feature values are delivered continuously.\n",
    "\n",
    "For now we will define only `batch_source` for both `driver_statistics` and `driver_trips`, and demonstrate the usage of `stream_source` in later part of the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the location we're using for the offline feature store.\n",
    "\n",
    "import os\n",
    "demo_data_location = os.path.join(os.getenv(\"FEAST_SPARK_STAGING_LOCATION\", \"file:///home/jovyan/\"), \"test_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_statistics_source_uri = os.path.join(demo_data_location, \"driver_statistics\")\n",
    "\n",
    "driver_statistics = FeatureTable(\n",
    "    name = \"driver_statistics\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = [\n",
    "        acc_rate,\n",
    "        conv_rate,\n",
    "        avg_daily_trips\n",
    "    ],\n",
    "    batch_source=FileSource(\n",
    "        event_timestamp_column=\"datetime\",\n",
    "        created_timestamp_column=\"created\",\n",
    "        file_format=ParquetFormat(),\n",
    "        file_url=driver_statistics_source_uri,\n",
    "        date_partition_column=\"date\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_trips_source_uri = os.path.join(demo_data_location, \"driver_trips\")\n",
    "\n",
    "\n",
    "driver_trips = FeatureTable(\n",
    "    name = \"driver_trips\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = [\n",
    "        trips_today\n",
    "    ],\n",
    "    batch_source=FileSource(\n",
    "        event_timestamp_column=\"datetime\",\n",
    "        created_timestamp_column=\"created\",\n",
    "        file_format=ParquetFormat(),\n",
    "        file_url=driver_trips_source_uri,\n",
    "        date_partition_column=\"date\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering entities and feature tables in Feast Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.apply(driver_id)\n",
    "client.apply(driver_statistics)\n",
    "client.apply(driver_trips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec:\n",
      "  name: driver_statistics\n",
      "  entities:\n",
      "  - driver_id\n",
      "  features:\n",
      "  - name: acc_rate\n",
      "    valueType: FLOAT\n",
      "  - name: conv_rate\n",
      "    valueType: FLOAT\n",
      "  - name: avg_daily_trips\n",
      "    valueType: INT32\n",
      "  batchSource:\n",
      "    type: BATCH_FILE\n",
      "    eventTimestampColumn: datetime\n",
      "    datePartitionColumn: date\n",
      "    createdTimestampColumn: created\n",
      "    fileOptions:\n",
      "      fileFormat:\n",
      "        parquetFormat: {}\n",
      "      fileUrl: file:///home/jovyan/test_data/driver_statistics\n",
      "meta:\n",
      "  createdTimestamp: '2021-03-07T22:21:45Z'\n",
      "\n",
      "spec:\n",
      "  name: driver_trips\n",
      "  entities:\n",
      "  - driver_id\n",
      "  features:\n",
      "  - name: trips_today\n",
      "    valueType: INT32\n",
      "  batchSource:\n",
      "    type: BATCH_FILE\n",
      "    eventTimestampColumn: datetime\n",
      "    datePartitionColumn: date\n",
      "    createdTimestampColumn: created\n",
      "    fileOptions:\n",
      "      fileFormat:\n",
      "        parquetFormat: {}\n",
      "      fileUrl: file:///home/jovyan/test_data/driver_trips\n",
      "meta:\n",
      "  createdTimestamp: '2021-03-07T22:21:45Z'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(client.get_feature_table(\"driver_statistics\").to_yaml())\n",
    "print(client.get_feature_table(\"driver_trips\").to_yaml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating batch source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feast is agnostic to how the batch source is populated, as long as it complies to the Feature Table specification. Therefore, any existing ETL tools can be used for the purpose of data ingestion. Alternatively, you can also use Feast SDK to ingest a Panda Dataframe to the batch source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_entities():\n",
    "    return np.random.choice(999999, size=100, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trips(entities):\n",
    "    df = pd.DataFrame(columns=[\"driver_id\", \"trips_today\", \"datetime\", \"created\"])\n",
    "    df['driver_id'] = entities\n",
    "    df['trips_today'] = np.random.randint(0, 1000, size=100).astype(np.int32)\n",
    "    df['datetime'] = pd.to_datetime(\n",
    "            np.random.randint(\n",
    "                datetime(2020, 10, 10).timestamp(),\n",
    "                datetime(2020, 10, 20).timestamp(),\n",
    "                size=100),\n",
    "        unit=\"s\"\n",
    "    )\n",
    "    df['created'] = pd.to_datetime(datetime.now())\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stats(entities):\n",
    "    df = pd.DataFrame(columns=[\"driver_id\", \"conv_rate\", \"acc_rate\", \"avg_daily_trips\", \"datetime\", \"created\"])\n",
    "    df['driver_id'] = entities\n",
    "    df['conv_rate'] = np.random.random(size=100).astype(np.float32)\n",
    "    df['acc_rate'] = np.random.random(size=100).astype(np.float32)\n",
    "    df['avg_daily_trips'] = np.random.randint(0, 1000, size=100).astype(np.int32)\n",
    "    df['datetime'] = pd.to_datetime(\n",
    "            np.random.randint(\n",
    "                datetime(2020, 10, 10).timestamp(),\n",
    "                datetime(2020, 10, 20).timestamp(),\n",
    "                size=100),\n",
    "        unit=\"s\"\n",
    "    )\n",
    "    df['created'] = pd.to_datetime(datetime.now())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)\n",
    "entities = generate_entities()\n",
    "stats_df = generate_stats(entities)\n",
    "trips_df = generate_trips(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing temporary file(s)...\n",
      "Data has been successfully ingested into FeatureTable batch source.\n",
      "Removing temporary file(s)...\n",
      "Data has been successfully ingested into FeatureTable batch source.\n"
     ]
    }
   ],
   "source": [
    "time.sleep(5)\n",
    "client.ingest(driver_statistics, stats_df)\n",
    "client.ingest(driver_trips, trips_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Historical Retrieval For Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-in-time correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Point In Time](https://raw.githubusercontent.com/feast-dev/feast/master/examples/minimal/images/pit-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feast joins the features to the entities based on the following conditions:\n",
    "1. Entity primary key(s) value matches.\n",
    "2. Feature event timestamp is the closest match possible to the entity event timestamp,\n",
    "   but must not be more recent than the entity event timestamp, and the difference must\n",
    "   not be greater than the maximum age specified in the feature table, unless the maximum age is not specified.\n",
    "3. If more than one feature table rows satisfy condition 1 and 2, feature row with the\n",
    "   most recent created timestamp will be chosen.\n",
    "4. If none of the above conditions are satisfied, the feature rows will have null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "from pyarrow.parquet import ParquetDataset\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(uri):\n",
    "    parsed_uri = urlparse(uri)\n",
    "    if parsed_uri.scheme == \"file\":\n",
    "        return pd.read_parquet(parsed_uri.path)\n",
    "    elif parsed_uri.scheme == \"gs\":\n",
    "        fs = gcsfs.GCSFileSystem()\n",
    "        files = [\"gs://\" + path for path in fs.glob(uri + '/part-*')]\n",
    "        ds = ParquetDataset(files, filesystem=fs)\n",
    "        return ds.read().to_pandas()\n",
    "    elif parsed_uri.scheme == 's3':\n",
    "        import s3fs\n",
    "        fs = s3fs.S3FileSystem()\n",
    "        files = [\"s3://\" + path for path in fs.glob(uri + '/part-*')]\n",
    "        ds = ParquetDataset(files, filesystem=fs)\n",
    "        return ds.read().to_pandas()\n",
    "    elif parsed_uri.scheme == 'wasbs':\n",
    "        import adlfs\n",
    "        fs = adlfs.AzureBlobFileSystem(\n",
    "            account_name=os.getenv('FEAST_AZURE_BLOB_ACCOUNT_NAME'), account_key=os.getenv('FEAST_AZURE_BLOB_ACCOUNT_ACCESS_KEY')\n",
    "        )\n",
    "        uripath = parsed_uri.username + parsed_uri.path\n",
    "        files = fs.glob(uripath + '/part-*')\n",
    "        ds = ParquetDataset(files, filesystem=fs)\n",
    "        return ds.read().to_pandas()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported URL scheme {uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471125</td>\n",
       "      <td>2020-10-19 20:01:39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>369032</td>\n",
       "      <td>2020-10-19 20:10:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>377665</td>\n",
       "      <td>2020-10-18 15:12:26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423368</td>\n",
       "      <td>2020-10-18 13:31:29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157514</td>\n",
       "      <td>2020-10-18 19:46:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>663063</td>\n",
       "      <td>2020-10-19 22:39:19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>673981</td>\n",
       "      <td>2020-10-18 14:12:48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>722971</td>\n",
       "      <td>2020-10-18 11:12:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>476369</td>\n",
       "      <td>2020-10-18 19:23:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>298148</td>\n",
       "      <td>2020-10-18 15:49:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id     event_timestamp\n",
       "0     471125 2020-10-19 20:01:39\n",
       "1     369032 2020-10-19 20:10:30\n",
       "2     377665 2020-10-18 15:12:26\n",
       "3     423368 2020-10-18 13:31:29\n",
       "4     157514 2020-10-18 19:46:46\n",
       "5     663063 2020-10-19 22:39:19\n",
       "6     673981 2020-10-18 14:12:48\n",
       "7     722971 2020-10-18 11:12:53\n",
       "8     476369 2020-10-18 19:23:05\n",
       "9     298148 2020-10-18 15:49:03"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "entities_with_timestamp = pd.DataFrame(columns=['driver_id', 'event_timestamp'])\n",
    "entities_with_timestamp['driver_id'] = np.random.choice(entities, 10, replace=False)\n",
    "entities_with_timestamp['event_timestamp'] = pd.to_datetime(np.random.randint(\n",
    "    datetime(2020, 10, 18).timestamp(),\n",
    "    datetime(2020, 10, 20).timestamp(),\n",
    "    size=10), unit='s')\n",
    "entities_with_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# get_historical_features will return immediately once the Spark job has been submitted succesfully.\n",
    "job = client.get_historical_features(\n",
    "    feature_refs=[\n",
    "        \"driver_statistics:avg_daily_trips\",\n",
    "        \"driver_statistics:conv_rate\",\n",
    "        \"driver_statistics:acc_rate\",\n",
    "        \"driver_trips:trips_today\"\n",
    "    ], \n",
    "    entity_source=entities_with_timestamp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_output_file_uri will block until the Spark job is completed.\n",
    "output_file_uri = job.get_output_file_uri()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>driver_statistics__acc_rate</th>\n",
       "      <th>driver_statistics__conv_rate</th>\n",
       "      <th>driver_statistics__avg_daily_trips</th>\n",
       "      <th>driver_trips__trips_today</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>471125</td>\n",
       "      <td>2020-10-19 20:01:39</td>\n",
       "      <td>0.474747</td>\n",
       "      <td>0.708197</td>\n",
       "      <td>111</td>\n",
       "      <td>324.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423368</td>\n",
       "      <td>2020-10-18 13:31:29</td>\n",
       "      <td>0.582376</td>\n",
       "      <td>0.073656</td>\n",
       "      <td>636</td>\n",
       "      <td>894.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>673981</td>\n",
       "      <td>2020-10-18 14:12:48</td>\n",
       "      <td>0.546367</td>\n",
       "      <td>0.896934</td>\n",
       "      <td>757</td>\n",
       "      <td>525.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>722971</td>\n",
       "      <td>2020-10-18 11:12:53</td>\n",
       "      <td>0.183651</td>\n",
       "      <td>0.819915</td>\n",
       "      <td>158</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>476369</td>\n",
       "      <td>2020-10-18 19:23:05</td>\n",
       "      <td>0.176206</td>\n",
       "      <td>0.667049</td>\n",
       "      <td>35</td>\n",
       "      <td>309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>298148</td>\n",
       "      <td>2020-10-18 15:49:03</td>\n",
       "      <td>0.453658</td>\n",
       "      <td>0.965575</td>\n",
       "      <td>72</td>\n",
       "      <td>779.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>377665</td>\n",
       "      <td>2020-10-18 15:12:26</td>\n",
       "      <td>0.069290</td>\n",
       "      <td>0.098833</td>\n",
       "      <td>918</td>\n",
       "      <td>635.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>369032</td>\n",
       "      <td>2020-10-19 20:10:30</td>\n",
       "      <td>0.268349</td>\n",
       "      <td>0.876938</td>\n",
       "      <td>803</td>\n",
       "      <td>781.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>663063</td>\n",
       "      <td>2020-10-19 22:39:19</td>\n",
       "      <td>0.814907</td>\n",
       "      <td>0.059869</td>\n",
       "      <td>338</td>\n",
       "      <td>965.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>157514</td>\n",
       "      <td>2020-10-18 19:46:46</td>\n",
       "      <td>0.027994</td>\n",
       "      <td>0.311030</td>\n",
       "      <td>650</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id     event_timestamp  driver_statistics__acc_rate  \\\n",
       "0     471125 2020-10-19 20:01:39                     0.474747   \n",
       "1     423368 2020-10-18 13:31:29                     0.582376   \n",
       "2     673981 2020-10-18 14:12:48                     0.546367   \n",
       "3     722971 2020-10-18 11:12:53                     0.183651   \n",
       "4     476369 2020-10-18 19:23:05                     0.176206   \n",
       "5     298148 2020-10-18 15:49:03                     0.453658   \n",
       "6     377665 2020-10-18 15:12:26                     0.069290   \n",
       "7     369032 2020-10-19 20:10:30                     0.268349   \n",
       "8     663063 2020-10-19 22:39:19                     0.814907   \n",
       "9     157514 2020-10-18 19:46:46                     0.027994   \n",
       "\n",
       "   driver_statistics__conv_rate  driver_statistics__avg_daily_trips  \\\n",
       "0                      0.708197                                 111   \n",
       "1                      0.073656                                 636   \n",
       "2                      0.896934                                 757   \n",
       "3                      0.819915                                 158   \n",
       "4                      0.667049                                  35   \n",
       "5                      0.965575                                  72   \n",
       "6                      0.098833                                 918   \n",
       "7                      0.876938                                 803   \n",
       "8                      0.059869                                 338   \n",
       "9                      0.311030                                 650   \n",
       "\n",
       "   driver_trips__trips_today  \n",
       "0                      324.0  \n",
       "1                      894.0  \n",
       "2                      525.0  \n",
       "3                        NaN  \n",
       "4                      309.0  \n",
       "5                      779.0  \n",
       "6                      635.0  \n",
       "7                      781.0  \n",
       "8                      965.0  \n",
       "9                      112.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "read_parquet(output_file_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieved result can now be used for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populating Online Storage with Batch Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to populate the online storage, we can use Feast SDK to start a Spark batch job which will extract the features from the batch source, then load the features to an online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "time.sleep(10)\n",
    "job = client.start_offline_to_online_ingestion(\n",
    "    driver_statistics,\n",
    "    datetime(2020, 10, 10),\n",
    "    datetime(2020, 10, 20)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SparkJobStatus.IN_PROGRESS: 1>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It will take some time before the Spark Job is completed\n",
    "time.sleep(15)\n",
    "job.get_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the job is completed, the SDK can be used to retrieve the result from the online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'driver_id': 884932},\n",
       " {'driver_id': 397135},\n",
       " {'driver_id': 471125},\n",
       " {'driver_id': 477225},\n",
       " {'driver_id': 395171},\n",
       " {'driver_id': 637490},\n",
       " {'driver_id': 574137},\n",
       " {'driver_id': 660456},\n",
       " {'driver_id': 616173},\n",
       " {'driver_id': 776855}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(15)\n",
    "entities_sample = np.random.choice(entities, 10, replace=False)\n",
    "entities_sample = [{\"driver_id\": e} for e in entities_sample]\n",
    "entities_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'driver_id': [884932,\n",
       "  397135,\n",
       "  471125,\n",
       "  477225,\n",
       "  395171,\n",
       "  637490,\n",
       "  574137,\n",
       "  660456,\n",
       "  616173,\n",
       "  776855],\n",
       " 'driver_statistics:avg_daily_trips': [473,\n",
       "  90,\n",
       "  111,\n",
       "  9,\n",
       "  670,\n",
       "  593,\n",
       "  849,\n",
       "  566,\n",
       "  874,\n",
       "  977]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(15)\n",
    "features = client.get_online_features(\n",
    "    feature_refs=[\"driver_statistics:avg_daily_trips\"],\n",
    "    entity_rows=entities_sample).to_dict()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>driver_id</th>\n",
       "      <th>driver_statistics:avg_daily_trips</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>884932</td>\n",
       "      <td>473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>397135</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>471125</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>477225</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>395171</td>\n",
       "      <td>670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>637490</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>574137</td>\n",
       "      <td>849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>660456</td>\n",
       "      <td>566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>616173</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>776855</td>\n",
       "      <td>977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   driver_id  driver_statistics:avg_daily_trips\n",
       "0     884932                                473\n",
       "1     397135                                 90\n",
       "2     471125                                111\n",
       "3     477225                                  9\n",
       "4     395171                                670\n",
       "5     637490                                593\n",
       "6     574137                                849\n",
       "7     660456                                566\n",
       "8     616173                                874\n",
       "9     776855                                977"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(15)\n",
    "pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features can now be used as an input to the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion from Streaming (real-time) Source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a streaming source, we can use Feast SDK to launch a Spark streaming job that continuously update the online store. First, we will update `driver_trips` feature table such that a new streaming source is added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: confluent_kafka in /opt/conda/lib/python3.8/site-packages (1.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import time\n",
    "import json\n",
    "import pytz\n",
    "import io\n",
    "import avro.schema\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "from pyarrow.parquet import ParquetDataset\n",
    "from urllib.parse import urlparse\n",
    "from datetime import datetime\n",
    "from avro.io import BinaryEncoder, DatumWriter\n",
    "from confluent_kafka import Producer\n",
    "from feast import Client, Feature, Entity, ValueType, FeatureTable\n",
    "from feast.data_source import FileSource, KafkaSource\n",
    "from feast.data_format import ParquetFormat, AvroFormat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting kafka brokers\n",
      "tcp://10.79.242.9:9092\n"
     ]
    }
   ],
   "source": [
    "# Change this to any Kafka broker addresses which is accessible by the spark cluster\n",
    "time.sleep(15)\n",
    "KAFKA_BROKER = os.getenv(\"DEMO_KAFKA_BROKERS\", os.environ[\"FEAST_RELEASE_KAFKA_PORT_9092_TCP\"])\n",
    "print('starting kafka brokers')\n",
    "print(KAFKA_BROKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_schema_json = json.dumps({\n",
    "    \"type\": \"record\",\n",
    "    \"name\": \"DriverTrips\",\n",
    "    \"fields\": [\n",
    "        {\"name\": \"driver_id\", \"type\": \"long\"},\n",
    "        {\"name\": \"trips_today\", \"type\": \"int\"},\n",
    "        {\n",
    "            \"name\": \"datetime\",\n",
    "            \"type\": {\"type\": \"long\", \"logicalType\": \"timestamp-micros\"},\n",
    "        },\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply driver_trips\n"
     ]
    }
   ],
   "source": [
    "time.sleep(30)\n",
    "driver_trips.stream_source = KafkaSource(\n",
    "    event_timestamp_column=\"datetime\",\n",
    "    created_timestamp_column=\"datetime\",\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    topic=\"driver_trips\",\n",
    "    message_format=AvroFormat(avro_schema_json)\n",
    ")\n",
    "print('apply driver_trips')\n",
    "client.apply(driver_trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the streaming job and send avro record to Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_stream_to_online_ingestion\n"
     ]
    }
   ],
   "source": [
    "time.sleep(30)\n",
    "print('start_stream_to_online_ingestion')\n",
    "job = client.start_stream_to_online_ingestion(\n",
    "    driver_trips\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_avro_record_to_kafka(topic, record):\n",
    "    value_schema = avro.schema.parse(avro_schema_json)\n",
    "    writer = DatumWriter(value_schema)\n",
    "    bytes_writer = io.BytesIO()\n",
    "    encoder = BinaryEncoder(bytes_writer)\n",
    "    writer.write(record, encoder)\n",
    "    \n",
    "    producer = Producer({\n",
    "        \"bootstrap.servers\": KAFKA_BROKER,\n",
    "    })\n",
    "    producer.produce(topic=topic, value=bytes_writer.getvalue())\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setup kafka config\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n",
      "sending a record\n"
     ]
    }
   ],
   "source": [
    "# Note: depending on the Kafka configuration you may need to create the Kafka topic first, like below:\n",
    "print('setup kafka config')\n",
    "# from confluent_kafka.admin import AdminClient, NewTopic\n",
    "# admin = AdminClient({'bootstrap.servers': KAFKA_BROKER})\n",
    "# new_topic = NewTopic('driver_trips', num_partitions=1, replication_factor=3)\n",
    "# admin.create_topics(new_topic)\n",
    "time.sleep(15)\n",
    "for record in trips_df.drop(columns=['created']).to_dict('record'):\n",
    "    print('sending a record')\n",
    "    time.sleep(1)\n",
    "    record[\"datetime\"] = (\n",
    "        record[\"datetime\"].to_pydatetime().replace(tzinfo=pytz.utc)\n",
    "    )\n",
    "\n",
    "    send_avro_record_to_kafka(topic=\"driver_trips\", record=record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving joined features from several feature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(15)\n",
    "entities_sample = np.random.choice(entities, 10, replace=False)\n",
    "print('retrieve features entities_sample')\n",
    "entities_sample = [{\"driver_id\": e} for e in entities_sample]\n",
    "entities_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('retrieve features')\n",
    "features = client.get_online_features(\n",
    "    feature_refs=[\"driver_statistics:avg_daily_trips\", \"driver_trips:trips_today\"],\n",
    "    entity_rows=entities_sample).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(5)\n",
    "print('print all features')\n",
    "pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will stop the streaming job\n",
    "job.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
