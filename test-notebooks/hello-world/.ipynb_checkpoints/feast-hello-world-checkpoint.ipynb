{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feast Hello World notebook\n",
    "This notebook is a step-by-step tutorial on how to use Feast and how it works. Read the comments and you'll be fine.  \n",
    "First part is about the offline store. Second part is about online store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feast\n",
    "from feast import Client\n",
    "from feast.entity import Entity\n",
    "from feast.value_type import ValueType\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.1.dev0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feast.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Offline store\n",
    "Offline store is for storing and retrieving data for training. Even if you update data there, you can still get old versions of your features, so that you're able to recreate your experiments.\n",
    "\n",
    "## Connect to Feast Core and define our data schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "Connection timed out while attempting to connect to core:6565",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFutureTimeoutError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/feast/sdk/python/feast/grpc/grpc.py\u001b[0m in \u001b[0;36mcreate_grpc_channel\u001b[0;34m(url, enable_ssl, enable_auth, ssl_server_cert_path, auth_metadata_plugin, timeout)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchannel_ready_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/grpc/_utilities.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/grpc/_utilities.py\u001b[0m in \u001b[0;36m_block\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                             \u001b[0;32mraise\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFutureTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFutureTimeoutError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-95f98509d215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"core:6565\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtelemetry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_feature_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/feast/sdk/python/feast/client.py\u001b[0m in \u001b[0;36mlist_feature_tables\u001b[0;34m(self, project, labels)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0;31m# Get latest feature tables from Feast Core\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m         feature_table_protos = self._core_service.ListFeatureTables(\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0mListFeatureTablesRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_grpc_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         )  # type: ListFeatureTablesResponse\n",
      "\u001b[0;32m/feast/sdk/python/feast/client.py\u001b[0m in \u001b[0;36m_core_service\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \"\"\"\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_core_service_stub\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             channel = create_grpc_channel(\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0menable_ssl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetboolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCORE_ENABLE_SSL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/feast/sdk/python/feast/grpc/grpc.py\u001b[0m in \u001b[0;36mcreate_grpc_channel\u001b[0;34m(url, enable_ssl, enable_auth, ssl_server_cert_path, auth_metadata_plugin, timeout)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mchannel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFutureTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         raise ConnectionError(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;34mf\"Connection timed out while attempting to connect to {url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         )\n",
      "\u001b[0;31mConnectionError\u001b[0m: Connection timed out while attempting to connect to core:6565"
     ]
    }
   ],
   "source": [
    "c = feast.Client(core_url=\"core:6565\", telemetry=False)\n",
    "c.list_feature_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An Entity is like a private key in a database. We'll be managing a taxi company with some taxi drivers.\n",
    "driver_id = feast.entity.Entity(\n",
    "    name=\"driver_id\", \n",
    "    description=\"Driver identifier\", \n",
    "    value_type=ValueType.INT64\n",
    ")\n",
    "\n",
    "# These are feature definitions. We'll have two features describing each driver.\n",
    "driver_name = feast.feature.Feature(\"name\", dtype=feast.value_type.ValueType.STRING)\n",
    "driver_surname = feast.feature.Feature(\"surname\", dtype=feast.value_type.ValueType.STRING)\n",
    "\n",
    "# This defines where Feast will store the data. Despite its name, it's not a source of the raw data.\n",
    "# IMO a better name would be `batch_storage` or `batch_storage_backend`.\n",
    "batch_source = feast.data_source.FileSource(\n",
    "    file_format=feast.data_format.ParquetFormat(),  # how internal data will be stored\n",
    "    file_url=\"file:///home/jovyan/work/feast/driver_batch.parquet\",  # where internal data will be stored\n",
    "    event_timestamp_column=\"event_timestamp\",  # column defining when an event happened/when information started being true\n",
    "    created_timestamp_column=\"created_timestamp\",  # column defining when we created/ingested the record\n",
    ")\n",
    "\n",
    "# A feature table is like a database table. Different tables can share the same entity (aka private key).\n",
    "# We'll be saving and retrieving our features from this table.\n",
    "driver_info = feast.feature_table.FeatureTable(\n",
    "    name = \"driver_info\",\n",
    "    entities = [\"driver_id\"],\n",
    "    features = [\n",
    "        driver_name,\n",
    "        driver_surname,\n",
    "    ],\n",
    "    batch_source=batch_source,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must apply definitions of enitites and feature tables. We then check that Feast created our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.apply(driver_id)\n",
    "c.apply(driver_info)\n",
    "c.list_feature_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest the data\n",
    "\n",
    "Say we obtained information about two drivers. Alice was hired on January 1st, Bob was hired on January 2nd, hence their different `event_timestamp`s. But we're ingesting this data only on January 2nd, hence same `created_timestamp`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_info_df = pd.DataFrame({\n",
    "    \"driver_id\": [1, 2],\n",
    "    \"name\": [\"Alice\", \"Bob\"],\n",
    "    \"surname\": [\"Aoe\", \"Boe\"],\n",
    "    \"event_timestamp\": [dt.datetime(2020, 1, 1), dt.datetime(2020, 1, 2)],\n",
    "    \"created_timestamp\": [dt.datetime(2020, 1, 2)] * 2,\n",
    "})\n",
    "driver_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ingest(driver_info, driver_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the data\n",
    "\n",
    "Entity dataframe used for retrieval (`entities_of_interest` in our case) can be thought of as a request: \"give me the state of entity X as it was on this date\". We can specify different timestamp for different retrieved entities. Notice that event timestamp in the result is set to the timestamp of retrieval; not the one of the source data. This makes sense because the result dataframe shows the state of requested entities _at requested timestamps_.\n",
    "\n",
    "Now we're gonna retrieve the state of the world for both our drivers as it was on January 1st."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_of_interest = pd.DataFrame(\n",
    "    {\n",
    "        \"driver_id\": [1, 2],\n",
    "        \"event_timestamp\": [dt.datetime(2020, 1, 1)] * 2,\n",
    "    }\n",
    ")\n",
    "entities_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a helper function that waits for Spark job to be finished.\n",
    "def wait_for_job(job):\n",
    "    start = dt.datetime.now()\n",
    "    print(job.get_status().name, end=\"\")\n",
    "    while job.get_status().name != \"COMPLETED\":\n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(0.5)\n",
    "    print(job.get_status().name)\n",
    "    print(\"The job took\", dt.datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\"], \n",
    "    entity_source=entities_of_interest, \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the driver with ID 2 has no name. That's because its event_timestamp in the data source \n",
    "is after the timestamp we specified during retrieval (at the requested time of retirieval \n",
    "this information wasn't yet known). This is called _point-in-time correction_.\n",
    "\n",
    "Let's try retrieving the state at January 2nd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\"], \n",
    "    entity_source=entities_of_interest.assign(event_timestamp=dt.datetime(2020, 1, 2)), \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now second driver's name is present too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appending data and retrieving it\n",
    "\n",
    "### Case 1: ingesting data that contains the old records\n",
    "\n",
    "Now our raw data source has been updated and contains more records. Old records weren't updated and are still there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_driver_info_df = driver_info_df.append(\n",
    "    pd.DataFrame({\n",
    "        \"driver_id\": [3, 4],\n",
    "        \"name\": [\"Celine\", \"Daniel\"],\n",
    "        \"surname\": [\"Coe\", \"Doe\"],\n",
    "        \"created_timestamp\": [dt.datetime(2020, 1, 3)] * 2,\n",
    "        \"event_timestamp\": [dt.datetime(2020, 1, 3)] * 2,\n",
    "    }),\n",
    "    ignore_index=True,\n",
    ")\n",
    "more_driver_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ingest(driver_info, more_driver_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve for all the drivers, but before we hired new ones (i.e. as of January 2nd). The new drivers should be None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_entities_of_interest = (\n",
    "    entities_of_interest\n",
    "    .append(pd.DataFrame({\"driver_id\": [3, 4]}), ignore_index=True)\n",
    "    .assign(event_timestamp=dt.datetime(2020, 1, 2))\n",
    ")\n",
    "more_entities_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\"], \n",
    "    entity_source=more_entities_of_interest, \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)\n",
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrieve them as of January 3rd:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\"], \n",
    "    entity_source=more_entities_of_interest.assign(event_timestamp=dt.datetime(2020, 1, 3)), \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)\n",
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Appending only new data, without old records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_driver_info_df = pd.DataFrame({\n",
    "    \"driver_id\": [5, 6],\n",
    "    \"name\": [\"Elisabeth\", \"Frank\"],\n",
    "    \"surname\": [\"Eoe\", \"Foe\"],\n",
    "    \"created_timestamp\": [dt.datetime(2020, 1, 4)] * 2,\n",
    "    \"event_timestamp\": [dt.datetime(2020, 1, 4)] * 2,\n",
    "})\n",
    "new_driver_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ingest(driver_info, new_driver_info_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_entities_of_interest = (\n",
    "    more_entities_of_interest\n",
    "    .append(pd.DataFrame({\"driver_id\": [5, 6]}), ignore_index=True)\n",
    "    .assign(event_timestamp=dt.datetime(2020, 1, 4))\n",
    ")\n",
    "new_entities_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\"], \n",
    "    entity_source=new_entities_of_interest, \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)\n",
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, we still have all the records, including the old ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating records\n",
    "\n",
    "Let's say Celine got married and she chagned her surname from Coe to Coe-Joe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_celine_info_df = pd.DataFrame({\n",
    "    \"driver_id\": [3],\n",
    "    \"name\": [\"Celine\"],\n",
    "    \"surname\": [\"Coe-Joe\"],\n",
    "    \"created_timestamp\": [dt.datetime(2020, 1, 5)],\n",
    "    \"event_timestamp\": [dt.datetime(2020, 1, 5)],\n",
    "})\n",
    "updated_celine_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ingest(driver_info, updated_celine_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get the old data for the timestamp before her wedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\", \"driver_info:surname\"], \n",
    "    entity_source=pd.DataFrame({\n",
    "        \"driver_id\": [3],\n",
    "        \"event_timestamp\": [dt.datetime(2020, 1, 4)],  # before wedding\n",
    "    }), \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)\n",
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, we got a historical value of this feature.\n",
    "\n",
    "Let's get her data as it was a week after her wedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\", \"driver_info:surname\"], \n",
    "    entity_source=pd.DataFrame({\n",
    "        \"driver_id\": [3],\n",
    "        \"event_timestamp\": [dt.datetime(2020, 1, 12)],  # after wedding\n",
    "    }), \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)\n",
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we got a value after the change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overwriting wrong data\n",
    "\n",
    "Now suppose we made a mistake. Celine's surname after her wedding is not Coe-Joe, but Coe-Zoe. We realised only two weeks after we ingested wrong data. We will ingest a new record with the same event_timestamp (because she still changed her surname on 5th of January) but a new created_timestamp (because that's when we fix the data).\n",
    "\n",
    "For comparison, we previously updated Celine with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_celine_info_df  # wrong data we used previously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now we will update her with this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_updated_celine_info_df = updated_celine_info_df.assign(\n",
    "    surname=\"Coe-Zoe\", \n",
    "    created_timestamp=dt.datetime(2020, 1, 19)  # we realised late about the mistake\n",
    ")\n",
    "fixed_updated_celine_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ingest(driver_info, fixed_updated_celine_info_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll try getting Celine's data as it was one week after the wedding. That is, **after she changed her name, but before we fixed the data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.get_historical_features(\n",
    "    [\"driver_info:name\", \"driver_info:surname\"], \n",
    "    entity_source=pd.DataFrame({\n",
    "        \"driver_id\": [3],\n",
    "        \"event_timestamp\": [dt.datetime(2020, 1, 12)],  # one week after wedding, before we fixed the data\n",
    "    }), \n",
    "    output_location=\"file:///home/jovyan/work/output.parquet\",\n",
    ")\n",
    "\n",
    "wait_for_job(job)\n",
    "output_df = pd.read_parquet(\"/home/jovyan/work/output.parquet\")\n",
    "output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output contains **fixed** data. This is the true state of the world as it was on the requested date. \n",
    "\n",
    "When ingesting the fix, we used the same `event_timestamp` as we used when ingesting the update in the first place. Feast therefore uses the record with newer `create_timestamp` (that is the fixed record in our case) while retrieving the data.\n",
    "\n",
    "If we wanted this to return the wrong data from before the fix, we should have ingested the fix with a different (newer) `event_timestamp` to pretend that the change happened later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Online store\n",
    "\n",
    "Online store allows getting a subset of data with low latency.\n",
    "\n",
    "In the Part 1 our jobs for retrieving the data took quite some time. Now we'll ingest the data to an online store and retrieve it from there. This is useful if you need the features at inference time - meaning you need them quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = c.start_offline_to_online_ingestion(\n",
    "    driver_info,\n",
    "    dt.datetime(2020, 1, 1),\n",
    "    dt.datetime(2020, 2, 1),\n",
    ")\n",
    "wait_for_job(job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The API for retrieving the data from online store is a bit different from the API for online store. Here's how we define our entities of interest and how we request the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities_of_interest_online = [\n",
    "    {\"driver_id\": i}\n",
    "    for i in range(1, 7)\n",
    "]\n",
    "entities_of_interest_online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "response = c.get_online_features(\n",
    "    [\"driver_info:name\", \"driver_info:surname\"],\n",
    "    entities_of_interest_online,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we got the response very quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(response.to_dict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
